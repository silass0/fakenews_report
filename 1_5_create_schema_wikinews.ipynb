{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filepath = '/home/silas/Documents/DataScience Notebooks/scraping/WikiNews/scrape_wikinews.csv'\n",
    "df = pd.read_csv(filepath,\n",
    "                names = ['id','title','url','date_written','keywords','content'],\n",
    "                dtype={'id':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str -> list\n",
    "df['keywords'] =df['keywords'].apply(lambda x: re.findall(r\"'([^']+)'\",x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSV-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dicts():\n",
    "    \n",
    "    def makeDateDict():\n",
    "        date_lst = list(df['date_written'].unique())\n",
    "        date_lst.remove('None')\n",
    "        return {elm:idx for idx,elm in enumerate(date_lst)} \n",
    "    \n",
    "    def makeKeywordDict():\n",
    "        st = set()\n",
    "        for elm in df['keywords']:\n",
    "            if elm != None: st.update(elm)\n",
    "        return {elm:idx for idx,elm in enumerate(st)}\n",
    "\n",
    "    return makeDateDict(), makeKeywordDict(),\n",
    "\n",
    "#get dicts\n",
    "date_dict, keyword_dict = make_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article csv\n",
    "def make_article_csv():\n",
    "    article_df = pd.DataFrame(\n",
    "        data={'id':list(df.id),\n",
    "              'time_id':list(map(lambda x: str(date_dict[x]) if x != \"None\" else None,list(df.date_written)))\n",
    "             }\n",
    "                            )\n",
    "    article_df = article_df.join(df[['id','url','content','title']].set_index('id'),on='id')\n",
    "    article_df.to_csv('wiki_article.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_simple_csv(dictx,filename):\n",
    "    frame = pd.DataFrame({'id':list(dictx.values()), 'value':list(dictx)})\n",
    "    frame.drop_duplicates().to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multi_csv(dictx,filename,column_name,id_name):\n",
    "    frame = pd.DataFrame(df[['id',column_name]])\n",
    "    frame = frame.dropna()\n",
    "    frame = frame.explode(column_name)\n",
    "    frame[column_name] = frame[column_name].apply(lambda x: dictx[x])\n",
    "    frame.drop_duplicates().to_csv(filename, index=False, header=['id',id_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_article_csv()\n",
    "\n",
    "make_simple_csv(date_dict,'wiki_time.csv')\n",
    "make_simple_csv(keyword_dict,'wiki_keyword.csv')\n",
    "\n",
    "make_multi_csv(keyword_dict,'wiki_keywords.csv','keywords','keyword_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execQuery(query,*printOption):\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"postgres\",\n",
    "                                      password = \"1234\",\n",
    "                                      host = \"localhost\",\n",
    "                                      port = \"5432\",\n",
    "                                      database = \"FakeNews\")\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(query)\n",
    "        record = cursor.fetchall()\n",
    "        return record\n",
    "    except (Exception, psycopg2.Error) as error :\n",
    "        connection = False\n",
    "        print (\"Error while connecting to PostgreSQL:\", error)\n",
    "    finally:\n",
    "        if(connection):\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            if not printOption:\n",
    "                print(\"Executed query and closed connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while connecting to PostgreSQL: no results to fetch\n"
     ]
    }
   ],
   "source": [
    "queryCreate = \"\"\"\n",
    "DROP TABLE IF EXISTS wiki_time CASCADE;\n",
    "DROP TABLE IF EXISTS wiki_article CASCADE;\n",
    "DROP TABLE IF EXISTS wiki_keywords CASCADE;\n",
    "DROP TABLE IF EXISTS wiki_keyword;\n",
    "\n",
    "CREATE TABLE wiki_time(\n",
    "    time_id int PRIMARY KEY,\n",
    "    time_stamp timestamp);\n",
    "\n",
    "CREATE TABLE wiki_article(\n",
    "    article_id int PRIMARY KEY,\n",
    "    time_id int REFERENCES wiki_time,\n",
    "    url varchar,\n",
    "    content varchar,\n",
    "    title varchar);\n",
    "    \n",
    "CREATE TABLE wiki_keyword(\n",
    "    keyword_id int PRIMARY KEY,\n",
    "    value varchar\n",
    "    );\n",
    "\n",
    "CREATE TABLE wiki_keywords(\n",
    "    article_id int REFERENCES wiki_article,\n",
    "    keyword_id int REFERENCES wiki_keyword,\n",
    "    PRIMARY KEY(article_id,keyword_id)\n",
    "    );\n",
    "    \n",
    "COPY wiki_time FROM '/home/silas/Documents/DataScience Notebooks/csv/wikiNews/wiki_time.csv' WITH (FORMAT csv, HEADER true);\n",
    "COPY wiki_article FROM '/home/silas/Documents/DataScience Notebooks/csv/wikiNews/wiki_article.csv' WITH (FORMAT csv, HEADER true);\n",
    "COPY wiki_keyword FROM '/home/silas/Documents/DataScience Notebooks/csv/wikiNews/wiki_keyword.csv' WITH (FORMAT csv, HEADER true);\n",
    "COPY wiki_keywords FROM '/home/silas/Documents/DataScience Notebooks/csv/wikiNews/wiki_keywords.csv' WITH (FORMAT csv, HEADER true);\n",
    "\"\"\"\n",
    "\n",
    "execQuery(queryCreate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is expected since the query creates and inserts, but does not return anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7474,)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#demonstrate database\n",
    "\n",
    "queryDemonstrate = \"\"\"\n",
    "SELECT count(distinct article_id) \n",
    "FROM wiki_time RIGHT JOIN\n",
    "    wiki_article using (time_id) INNER JOIN\n",
    "    wiki_keywords using (article_id) INNER JOIN\n",
    "    wiki_keyword using (keyword_id)\n",
    "LIMIT 5\"\"\"\n",
    "\n",
    "execQuery(queryDemonstrate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
